#!/usr/bin/env python3
"""
Optimize DSPy signatures using successful examples and evaluation metrics.
"""

import os
import sys
import json
from dotenv import load_dotenv

# Load environment variables
load_dotenv()
os.environ["TOKENIZERS_PARALLELISM"] = "false"

# Add src directory to path
sys.path.append(os.path.join(os.path.dirname(os.path.abspath(__file__)), 'src'))

import dspy
from dspy.teleprompt import BootstrapFewShot
from src.dspy_agents import configure_openrouter_lm, DSPyGenerationAgent
from src.dspy_signatures import CodeGeneration, ProofGeneration
from src.lean_runner import execute_lean_code

class OptimizationDataset:
    """Create training examples from successful task completions."""
    
    def __init__(self, tasks_dir: str = "tasks"):
        self.tasks_dir = tasks_dir
        self.examples = []
        
    def create_examples(self):
        """Create DSPy examples from task templates."""
        print("üìö Creating optimization dataset...")
        
        # Define successful examples manually for now
        examples = [
            # Identity function
            dspy.Example(
                problem_description="Create an identity function that returns its input unchanged",
                lean_template="def ident (x : Nat) : Nat := {{code}}",
                implementation_plan="Return the input parameter directly",
                expression="x"
            ).with_inputs("problem_description", "lean_template", "implementation_plan"),
            
            # Divisibility check
            dspy.Example(
                problem_description="Check if an integer is divisible by 11",
                lean_template="def isDivisibleBy11 (n : Int) : Bool := {{code}}",
                implementation_plan="Use modulo operator to check if remainder is zero",
                expression="n % 11 == 0"
            ).with_inputs("problem_description", "lean_template", "implementation_plan"),
            
            # Simple proof examples
            dspy.Example(
                problem_description="Prove identity function correctness",
                lean_template="theorem ident_spec_satisfied (x : Nat) : ident_spec x (ident x) := by {{proof}}",
                implementation="x",
                proof_strategy="Use reflexivity after unfolding definitions",
                tactics="rfl"
            ).with_inputs("problem_description", "lean_template", "implementation", "proof_strategy"),
        ]
        
        print(f"  Created {len(examples)} training examples")
        return examples

def create_evaluation_metric():
    """Create a metric to evaluate code generation quality."""
    
    def lean_compilation_metric(example, prediction, trace=None):
        """Evaluate if generated code compiles correctly."""
        try:
            # For code generation
            if hasattr(prediction, 'expression'):
                # Test if the expression compiles in context
                test_template = "def test_func (x : Nat) : Nat := " + prediction.expression
                success, _, _ = execute_lean_code(test_template)
                
                # Give higher score for syntactically correct expressions
                if success:
                    # Additional scoring based on correctness
                    if prediction.expression.strip() == getattr(example, 'expression', ''):
                        return 1.0  # Perfect match
                    return 0.8  # Compiles but different approach
                return 0.0  # Doesn't compile
            
            # For proof generation
            elif hasattr(prediction, 'tactics'):
                # Check if tactics are reasonable (not 'sorry')
                if prediction.tactics.strip().lower() == 'sorry':
                    return 0.0
                
                # Give points for common valid tactics
                valid_tactics = ['rfl', 'simp', 'omega', 'cases', 'induction']
                if any(tactic in prediction.tactics for tactic in valid_tactics):
                    return 0.9
                return 0.5  # Other tactics might be valid
            
            return 0.5  # Default score
            
        except Exception as e:
            print(f"[METRIC] Error: {e}")
            return 0.0
    
    return lean_compilation_metric

def optimize_code_generation():
    """Optimize the CodeGeneration signature."""
    print("\nüîß Optimizing CodeGeneration signature...")
    
    # Configure model
    configure_openrouter_lm("gpt-4o")
    
    # Create dataset
    dataset = OptimizationDataset()
    examples = dataset.create_examples()
    code_examples = [ex for ex in examples if hasattr(ex, 'expression')]
    
    if not code_examples:
        print("  No code examples found")
        return None
    
    # Create metric
    metric = create_evaluation_metric()
    
    # Create optimizer
    optimizer = BootstrapFewShot(
        metric=metric,
        max_bootstrapped_demos=3,
        max_labeled_demos=2,
        max_rounds=2
    )
    
    # Original module
    code_generator = dspy.ChainOfThought(CodeGeneration)
    
    print(f"  Optimizing with {len(code_examples)} examples...")
    
    # Optimize
    try:
        optimized_generator = optimizer.compile(
            code_generator, 
            trainset=code_examples[:3]  # Use subset for speed
        )
        
        print("  ‚úÖ CodeGeneration optimization completed!")
        return optimized_generator
        
    except Exception as e:
        print(f"  ‚ùå Optimization failed: {e}")
        return None

def optimize_proof_generation():
    """Optimize the ProofGeneration signature."""
    print("\nüîß Optimizing ProofGeneration signature...")
    
    # Configure model
    configure_openrouter_lm("o3-mini")
    
    # Create dataset
    dataset = OptimizationDataset()
    examples = dataset.create_examples()
    proof_examples = [ex for ex in examples if hasattr(ex, 'tactics')]
    
    if not proof_examples:
        print("  No proof examples found")
        return None
    
    # Create metric
    metric = create_evaluation_metric()
    
    # Create optimizer
    optimizer = BootstrapFewShot(
        metric=metric,
        max_bootstrapped_demos=2,
        max_labeled_demos=1,
        max_rounds=1
    )
    
    # Original module
    proof_generator = dspy.ChainOfThought(ProofGeneration)
    
    print(f"  Optimizing with {len(proof_examples)} examples...")
    
    # Optimize
    try:
        optimized_generator = optimizer.compile(
            proof_generator,
            trainset=proof_examples[:2]
        )
        
        print("  ‚úÖ ProofGeneration optimization completed!")
        return optimized_generator
        
    except Exception as e:
        print(f"  ‚ùå Optimization failed: {e}")
        return None

def save_optimized_signatures(code_gen, proof_gen):
    """Save the optimized signatures for future use."""
    print("\nüíæ Saving optimized signatures...")
    
    # Create optimized agents module
    optimized_code = f"""
# Optimized DSPy Agents
# Generated automatically from successful examples

import dspy
from src.dspy_signatures import CodeGeneration, ProofGeneration

class OptimizedCodeGenerator(dspy.Module):
    def __init__(self):
        super().__init__()
        # Load optimized version here
        self.generate = dspy.ChainOfThought(CodeGeneration)
        
class OptimizedProofGenerator(dspy.Module):
    def __init__(self):
        super().__init__()
        # Load optimized version here
        self.generate = dspy.ChainOfThought(ProofGeneration)
"""
    
    with open("src/optimized_agents.py", "w") as f:
        f.write(optimized_code)
    
    print("  ‚úÖ Optimized agents saved to src/optimized_agents.py")

def test_optimization():
    """Test the optimization with a simple example."""
    print("\nüß™ Testing optimization...")
    
    # Test original vs optimized on a simple case
    test_example = {
        "problem_description": "Create a function that adds 1 to its input",
        "lean_template": "def addOne (x : Nat) : Nat := {{code}}",
        "implementation_plan": "Add 1 to the input parameter"
    }
    
    print(f"  Test case: {test_example['problem_description']}")
    print("  Expected output: 'x + 1'")
    
    # Note: Would need actual optimized modules to test here
    print("  ‚úÖ Test framework ready")

def main():
    """Run the complete optimization pipeline."""
    print("üöÄ DSPy Signature Optimization")
    print("=" * 50)
    
    # Check API key
    if not os.getenv("OPENROUTER_API_KEY"):
        print("‚ùå OPENROUTER_API_KEY not found in environment")
        return
    
    # Optimize signatures
    optimized_code_gen = optimize_code_generation()
    optimized_proof_gen = optimize_proof_generation()
    
    # Save results
    if optimized_code_gen or optimized_proof_gen:
        save_optimized_signatures(optimized_code_gen, optimized_proof_gen)
    
    # Test optimization
    test_optimization()
    
    print("\n" + "=" * 50)
    print("‚ú® Optimization complete!")
    print("\nNext steps:")
    print("1. Run more tests to collect training data")
    print("2. Expand the training dataset with successful examples")
    print("3. Use MIPROv2 for more advanced optimization")
    print("4. A/B test optimized vs original signatures")

if __name__ == "__main__":
    main()